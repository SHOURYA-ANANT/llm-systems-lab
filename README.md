
<p align="center">
 <img width="1589" height="348" alt="llm_systems_lab_banner" src="https://github.com/user-attachments/assets/67d69822-de45-49b2-ab0d-216236b23813" />
</p>

<br>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10%2B-blue?style=for-the-badge&logo=python" />
  <img src="https://img.shields.io/badge/PyTorch-LLM%20Training-ee4c2c?style=for-the-badge&logo=pytorch" />
  <img src="https://img.shields.io/badge/Tokenizer-BPE-orange?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Transformer-Grok%2FLLaMA%20Architecture-green?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Agents-MCP%20%2F%20AAS-yellow?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Research-IEEE%20Track-purple?style=for-the-badge" />
</p>

# ğŸš€ LLM Systems Lab

Welcome to **LLM Systems Lab** â€” a hands-on journey into **modern LLM engineering**, covering everything from custom tokenizers to Grok/LLaMA-style transformer architectures, training pipelines, inference servers, and advanced agent systems.

This project captures a complete **learning + engineering + research** adventure, building LLM components from scratch with a strong focus on real-world readiness and scientific clarity.

---

## ğŸ§  What This Project Is About

- ğŸ”¡ Tokenizer Engineering (BPE, subword vocabularies)
- ğŸ§  Transformers from Scratch (attention â†’ MHA â†’ blocks â†’ full models)
- âš¡ Grok/LLaMA-style architecture upgrades (RoPE, RMSNorm, SwiGLU, GQA/MQA, FlashAttention)
- ğŸš€ Training pipelines (AdamW, LR schedules, FP16/BF16, checkpointing)
- ğŸ¤– Agent systems (MCP Tools, multi-step reasoning, AAS loops, RAG)
- ğŸ§ª Research (ablations, scaling curves, attention experiments)
- ğŸ–¥ Inference systems (FastAPI server, streaming tokens, KV cache)

---

# âš¡ Quick Start Guide

### ğŸ“¦ 1. Clone & Setup
```
git clone https://github.com/<your-username>/llm-systems-lab.git
cd llm-systems-lab
python -m venv .venv
source .venv/bin/activate     # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### ğŸ§ª 2. Run a tiny test model
```
python inference/generate.py --prompt "Hello LLM"
```

### ğŸ§  3. Train a small model
```
python training/trainer.py --config configs/toy_config.yml
```

### ğŸŒ 4. Launch inference server
```
uvicorn inference.server:app --host 0.0.0.0 --port 8000
```

### ğŸ“’ 5. Open Jupyter notebooks
```
jupyter notebook
```

---

# ğŸ¤ Connect With Me

- ğŸ”— LinkedIn: https://www.linkedin.com/in/YOUR-LINKEDIN-ID
- ğŸ¦ Twitter: https://twitter.com/YOUR-TWITTER
- âœï¸ Blog: https://YOUR-BLOG.com
- ğŸ’¬ Discord: https://discord.gg/YOUR-SERVER

---

# ğŸ‰ Letâ€™s Build Great Things

Thanks for checking out LLM Systems Lab â€” enjoy exploring, breaking, fixing, and understanding the internals of LLMs.
